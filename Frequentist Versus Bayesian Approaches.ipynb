{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacobaparker/qnc/blob/main/Frequentist%20Versus%20Bayesian%20Approaches.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyFv5eqQydE0"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PennNGG/Quantitative-Neuroscience/blob/master/Concepts/Python/Frequentist%20Versus%20Bayesian%20Approaches.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKIiY6p3GRFq"
      },
      "source": [
        "# Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7VmLUr5GTNw"
      },
      "source": [
        "Debates between frequentists and Bayesians have carried on for years, touching on issues that are in some cases very [practical](https://www.ejwagenmakers.com/2008/BayesFreqBook.pdf) and other cases much more [philosophical](http://www.stat.columbia.edu/~gelman/research/published/philosophy.pdf). The goal here is not to dive deeply into all of those debates but rather to introduce you to the basic issues, because they are at the heart of what we can and cannot do with statistics.\n",
        "\n",
        "More specifically, the two camps differ fundamentally on how to interpret randomness, which profoundly affects the kinds of inferences that can be drawn on the basis of noisy data:\n",
        "\n",
        "A **frequentist** thinks of probability only in terms of the frequency of many repeated events that include some element of randomness. **To a frequentist, assigning a probability to a singular event that can either happen or not happen, particularly one that is not directly or yet measured,  is nonsensical** (\"There is no place in our system for speculations concerning the probability that the sun will rise tomorrow\" -- William Feller). As a consequence of these ideas, a frequentist operates on the conditional distribution of the data, assuming that a hypothesis is true. That is, one makes a series of repeated measurements (the data) under fixed conditions, obtaining what is essentially a histogram. Inferences about the nature of the process that generated the data then allow only for this definition of randomness or uncertainty: the obtained variability in the data. Questions of the form \"What is the probability that process x generated my data?\" are undefined in this framework, because a probability cannot be assigned to an unknown and unseeable process (or \"hypothesis\"), only to repeated measures. Instead, the best you can do is simply assume that a particular process was the one that generated your data, and then ask \"What is the probability that I would have obtained my data, assuming that x was the true process?\" This question is the basis for null hypotheses (typically defined in terms of the parameters of the probability distribution that you would expect the data to be drawn from under a particular set of assumptions) and p-values: computing the likelihood p(data | null hypothesis).\n",
        "\n",
        "Benefits of this approach are that frequentist-based statistics are typically relatively easy to compute, they require few assumptions, and they tend to promote good experimental design (because you need to very carefully control the conditions under which the data are collected).\n",
        "\n",
        "Drawbacks include the fact that definitions of probability in this framework are often highly counter-intuitive to how most of us think, resulting in results that can be very difficult to interpret. A good example is the concept of a \"confidence interval\" in frequentist statistics, which is described nicely [here](https://jfiksel.github.io/2018-01-08-explaining-confidence-intervals/).\n",
        "\n",
        "A **Bayesian** thinks of probability as the degree of belief in a proposition (or hypothesis). In this framework, data represent evidence that can support or oppose such a belief, which is represented as a  probability distribution. Thus, unlike from the frequentist perspective, **from the Bayesian perspective it is perfectly natural to describe the belief (or probability) that particular values of particular parameters of a particular probability distribution (together encompassing a \"hypothesis\" about the data) are true**.\n",
        "\n",
        "These ideas are derived directly from the definition of joint probability (see [Independence and Lack Thereof](https://colab.research.google.com/drive/1YFwKKkWUjtV6_Nx2upNpFYHJNeXIeQB6) for a related discussion):\n",
        "\n",
        "$P(A\\cap B)=p(A|B)\\times p(B) = p(B|A)\\times p(A)$\n",
        "\n",
        "where $P(A\\cap B)$ is read as \"the probability that A and B are true\" and P(A | B) is read as \"the probability that A is true, given that B is true\" or just \"the probability of A given B.\"\n",
        "\n",
        "If we call A the Hypothesis and B the Data, and rearrange, we get Bayes' Rule:\n",
        "\n",
        "$P(Hypothesis|Data)=\\frac{P(Data|hypothesis)\\times P(Hypothesis)}{P(Data)}$\n",
        "\n",
        "Where *P*(*Hypothesis* | *Data*) is called the posterior probability (or just posterior), *P*(*Data* | *Hypothesis*) is the likelihood, *P*(*Hypothesis*) is the prior, and *P*(*Data*) is the marginal probability of the data.\n",
        "\n",
        "Benefits of the Bayesian approach are that it tends to get at the intuitive concepts that one is addressing (e.g., the probability that a hypothesis is true, given the data), and it does so in a rigorous manner.\n",
        "\n",
        "Drawbacks include questions about [how to identify an appropriate prior](https://stats.stackexchange.com/questions/78606/how-to-choose-prior-in-bayesian-parameter-estimation).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isP38xJSbJuA"
      },
      "source": [
        "# Tutorial and Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Eh2Pu8glb6C"
      },
      "source": [
        "To use this tutorial, read the text and then try to generate code to solve the exercises. Answers will be posted to GitHub after the class they are due.\n",
        "\n",
        "The learning objective is to gain insights into thinking about inference from a \"Frequentist\" versus a \"Bayesian\" perspective. In brief, because a Frequentist does not consider the probability of an event or state of the world or hypothesis, only their frequency of occurrance, it is not possible to ask questions of the form \"what is the probabilty that hypothesis x is true?\" Instead, one can only consider questions of the form, \"what is the probabilty that I would have obtained my data, given that hypothesis x is true?\" In contrast, Bayesians consider the probabilities of such things (often called the strength of belief), but doing so can require making assumptions that can be difficult to prove.\n",
        "\n",
        "Let's start with a simple example, taken from:\n",
        "\n",
        "https://en.wikipedia.org/wiki/Base_rate_fallacy#Example_1:_HIV\n",
        "\n",
        "\"Imagine running an HIV test on A SAMPLE of 1000 persons ...\"\n",
        "\n",
        "\"The test has a false positive rate of 5% (0.05)...\" i.e., the probability that someone who takes the test gets a POSITIVE result despite the fact that the person does NOT have HIV\n",
        "\n",
        "\"...and no false negative rate.\" i.e., The probability that someone who takes the test gets a NEGATIVE result despite the fact that the person DOES have HIV.\n",
        "\n",
        "Answers to the exercises below will be found [here](https://github.com/PennNGG/Quantitative-Neuroscience/tree/master/Answers%20to%20Exercises/Python) after the due date."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3hkaaTFlukb"
      },
      "source": [
        "### Exercise #1: If someone gets a positive test, is it \"statistically significant\" at the p<0.05 level? Why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If we treat the false positive rate as the probability of observing a positive test result when the null hypothesis (the person is HIV negative) is true, then a single positive test corresponds to p=0.05. Therefore, p is not <0.05.**"
      ],
      "metadata": {
        "id": "LPrEwLhH0k1b"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ItR0WXCmBgL"
      },
      "source": [
        "### Exercise #2: What is the probability that if someone gets a positive test, that person is infected?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The probability we are interesteed in corresponds to the posterior probability $p(H_1|X_1)$, where $H_1$ is the hypothesis that someone is infected and $X_1$ is a positive test result. We can write\n",
        "\n",
        "$$p(H_1|X_1) = \\frac{p(X_1|H_1)p(H_1)}{p(X_1)}.$$\n",
        "\n",
        "Since the false negative rate is 0, we know that $p(X_1|H_1) = 1$. We now have\n",
        "\n",
        "$$p(H_1|X_1) = \\frac{p(H_1)}{p(X_1)}.$$\n",
        "\n",
        "Let us assume that we want the prior probability $p(H_1)$, which corresponds to the incidence of HIV, to be a free variable. Using what we know about the false positive/negative rates, we can derive an expression that allows us to compute the posterior probability $p(H_1|X_1)$ from only the prior $p(H_1)$. To start, the false positive and false negative rates can be written as\n",
        "\n",
        "$$p(X_1|H_2) = 0.05, \\ p(X_2|H_1) = 0,$$\n",
        "\n",
        "respectively, where $H_2$ is the hypothesis that someone is not infected and $X_2$ is a negative test result. Using the definition of conditional and joint probabilities, we can write $p(X_1)$ as\n",
        "\n",
        "$$p(X_1) = \\frac{p(X_1:H_2)}{p(H_2|X_1)},$$\n",
        "\n",
        "then\n",
        "\n",
        "$$p(X_1) = \\frac{p(H_2)p(X_1|H_2)}{p(H_2|X_1)}.$$\n",
        "\n",
        "Since we have two mutually exclusive hypotheses, we can rewrite this as\n",
        "\n",
        "$$p(X_1) = \\frac{(1-p(H_1))(0.05)}{(1-p(H_1|X_1))}.$$\n",
        "\n",
        "Substituting into our original expression, we get\n",
        "\n",
        "$$p(H_1|X_1) = \\frac{p(H_1)}{\\frac{0.05(1-p(H_1))}{1-p(H_1|X_1)}}.$$\n",
        "\n",
        "Through a lot of rearranging, we finally get\n",
        "\n",
        "$$p(H_1|X_1) = \\frac{20p(H_1)}{1+19p(H_1)}.$$\n",
        "\n",
        "This expression can be used to compute the posterior probability knowing only the prior probability."
      ],
      "metadata": {
        "id": "l3xUHRxL1wFP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ7aAEoKmkTV"
      },
      "source": [
        "Following on Exercise #2, let's do the same thing, but this time we will try different values for the proportion of the population that is actually infected. What you should notice is that the **PROPORTION INFECTED GIVEN A POSITIVE TEST** depends (a lot!) on the **OVERALL RATE OF INFECTION**. Put another way, to determine the probabilty of a hypothesis, given your data (e.g., proportion infected given a positive test), you have to know the probability that the hypothesis was true without any data.\n",
        "\n",
        "Why is this the case? It is a simple consequence of the definition of a conditional probability, formulated as Bayes' Rule. Specifically, the joint probability of two events, call them A and B, is defined as: $$p(A\\,and\\,B) = p(A) \\times p(B\\,|\\,A)$$ $$p(B\\,and\\,A) = p(B) \\times p(A\\,|\\,B)$$\n",
        "\n",
        "Now, calling A the Hypothesis and B the Data, then rearranging, we get:$$p(Hypothesis\\,|\\,Data) = \\frac{p(Data\\,|\\,Hypothesis) \\times p(Hypothesis)}{p(Data)}$$\n",
        "\n",
        "So you cannot calculate the probability of the hypothesis, given the data (i.e., the Bayesian posterior), without knowing the probability of the hypothesis independent of any data (i.e., the prior).\n",
        "\n",
        "For this exercise, assume a range of priors (infection rates) from 0 to 1 in steps of 0.1."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Computed analytically, then simulations (copied from Diego because I spent too much time solving analytically) below.**"
      ],
      "metadata": {
        "id": "s19D1RLShVLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_pH1cX1(pH1):\n",
        "  return (20*pH1)/(1+19*pH1)\n",
        "\n",
        "pH1 = np.arange(0,1.1,0.1)\n",
        "pH1cX1 = compute_pH1cX1(pH1)\n",
        "\n",
        "for i in range(len(pH1)):\n",
        "  print(f'p(H1)={pH1[i]:.1f}, p(H1|X1)={pH1cX1[i]:.3f}')"
      ],
      "metadata": {
        "id": "J9XoVN4jF7-j",
        "outputId": "ee7af984-e9c1-4756-9ef0-e6e34a0b611c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p(H1)=0.0, p(H1|X1)=0.000\n",
            "p(H1)=0.1, p(H1|X1)=0.690\n",
            "p(H1)=0.2, p(H1|X1)=0.833\n",
            "p(H1)=0.3, p(H1|X1)=0.896\n",
            "p(H1)=0.4, p(H1|X1)=0.930\n",
            "p(H1)=0.5, p(H1|X1)=0.952\n",
            "p(H1)=0.6, p(H1|X1)=0.968\n",
            "p(H1)=0.7, p(H1|X1)=0.979\n",
            "p(H1)=0.8, p(H1|X1)=0.988\n",
            "p(H1)=0.9, p(H1|X1)=0.994\n",
            "p(H1)=1.0, p(H1|X1)=1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this code is from Diego\n",
        "\n",
        "import scipy.stats as st\n",
        "\n",
        "N = 10000\n",
        "false_positive_rate = 0.05 \n",
        "false_negative_rate = 0\n",
        "\n",
        "# Now we can do the same thing for the range 0->1\n",
        "infected_proportions = np.arange(0.0, 1.1, 0.1)\n",
        "for idx, val in enumerate(infected_proportions):\n",
        "    \n",
        "   # Simulate # infections in the SAMPLE, given the POPULATION rate\n",
        "   is_infected = st.binom.rvs(1, val, size=N)\n",
        "      \n",
        "   # Count the number infected\n",
        "   num_infected = is_infected.sum()\n",
        "   \n",
        "   # Make array of positive tests, given that falseNegativeRate=0 ...\n",
        "   is_positive = np.copy(is_infected)\n",
        "   \n",
        "   # And falsePositiveRate > 0\n",
        "   is_positive[is_infected==0] = st.binom.rvs(1, false_positive_rate, size=N-num_infected)\n",
        "   \n",
        "   # The probability that someone with a positive test is infected\n",
        "   p_is_infected_given_is_positive = (np.logical_and(is_infected==1, is_positive==1).sum())/is_positive.sum()\n",
        "   \n",
        "   # We can compute the Bayesian Posterior as:\n",
        "   # p(hypothesis | data) = (p(data | hypothesis) * p(hypothesis)) / p(data)\n",
        "   # Note that we are using the true rate from the full POPULATION, so these predictions will differ slightly from the probability computed above (pIsInfectedGivenIsPositiveTest) from the SAMPLE\n",
        "   p_data_given_hypothesis = 1 - false_negative_rate\n",
        "   p_hypothesis = val\n",
        "   p_data = is_positive.sum()/is_positive.size   \n",
        "   p_hypothesis_given_data = (p_data_given_hypothesis * p_hypothesis) / p_data\n",
        "\n",
        "   # Compute the theoretial posterior probability: \n",
        "   print(f'Infection rate={val:.1f}, proportion infected given a positive test={p_is_infected_given_is_positive:.3f}, Posterior={p_hypothesis_given_data:.3f}')"
      ],
      "metadata": {
        "id": "OlFzQCS_ZLtr",
        "outputId": "556f4f88-b6c6-478b-9d86-393745ea389f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Infection rate=0.0, proportion infected given a positive test=0.000, Posterior=0.000\n",
            "Infection rate=0.1, proportion infected given a positive test=0.707, Posterior=0.695\n",
            "Infection rate=0.2, proportion infected given a positive test=0.821, Posterior=0.829\n",
            "Infection rate=0.3, proportion infected given a positive test=0.890, Posterior=0.879\n",
            "Infection rate=0.4, proportion infected given a positive test=0.927, Posterior=0.930\n",
            "Infection rate=0.5, proportion infected given a positive test=0.954, Posterior=0.928\n",
            "Infection rate=0.6, proportion infected given a positive test=0.968, Posterior=0.976\n",
            "Infection rate=0.7, proportion infected given a positive test=0.980, Posterior=0.978\n",
            "Infection rate=0.8, proportion infected given a positive test=0.987, Posterior=0.993\n",
            "Infection rate=0.9, proportion infected given a positive test=0.994, Posterior=0.992\n",
            "Infection rate=1.0, proportion infected given a positive test=1.000, Posterior=1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tteEm2Qlgbb3"
      },
      "source": [
        "# Credits\n",
        "\n",
        "Copyright 2021 by Joshua I. Gold, University of Pennsylvania"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "pKIiY6p3GRFq",
        "J3hkaaTFlukb"
      ],
      "name": "Frequentist Versus Bayesian Approaches",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}